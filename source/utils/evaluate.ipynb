{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjJCGbbbXdRq",
        "outputId": "89287170-959c-4bc0-d9f8-dc8b745b1116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FYzB7vKcXWcs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andro\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from retrieval import Book, generate_embeddings, get_embedding_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CwkBeunoXWcv",
        "outputId": "0d5d9255-866b-4857-fb35-967a8d35be91"
      },
      "outputs": [],
      "source": [
        "hf_token = \"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\"\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", token=hf_token, torch_dtype=torch.bfloat16).to(device)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", token=hf_token, torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8w9fh8eOXWcv"
      },
      "outputs": [],
      "source": [
        "text_generator = pipeline('text-generation',\n",
        "                          model=model,\n",
        "                          tokenizer=tokenizer,\n",
        "                          torch_dtype=torch.bfloat16,\n",
        "                          device=0,\n",
        "                          # device_map=\"auto\",\n",
        "                          do_sample=True,\n",
        "                          top_k=10,\n",
        "                          num_return_sequences=1,\n",
        "                          max_length=3_000,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xrOUqpKdlFy",
        "outputId": "9bc9324c-4e4b-43d0-b1ca-fae595dba7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(model.device)\n",
        "print(text_generator.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eGCFS5-D87ZL"
      },
      "outputs": [],
      "source": [
        "class Character_chat:\n",
        "    def __init__(self, character_name, book, book_name, add_quotes=True):\n",
        "\n",
        "        self.character_name = character_name\n",
        "        self.system_prompt = f\"\"\"\n",
        "            You are {character_name}, a fictional character from {book_name}.\n",
        "            Respond in a single sentence. When answering questions, use the book excerpt enclosed by triple backquotes for context.\n",
        "            When you are asked a question or told something you must only respond in character.\n",
        "            The user's prompt begins with 'user:'\n",
        "            The character's reponses begin with '{character_name}:'\n",
        "            \"\"\"\n",
        "        self.lines = book.get_character_lines(character_name)\n",
        "        if add_quotes:\n",
        "          self.character_quotes = \"\\n\".join([f\"{character_name}: \" + line for line in self.lines[:10]])\n",
        "          self.system_prompt += f\"Here are some examples of {character_name}'s dialogue:\\n\"\n",
        "          self.system_prompt += self.character_quotes\n",
        "\n",
        "        # self.prompt = \"\"\"<s>[INST]\\n\n",
        "        #                 <<SYS>>\\n\n",
        "        #                   {system_prompt}\\n\n",
        "        #                 <</SYS>>\\n\\n\n",
        "        #                 user:{user_input}\\n\\n\n",
        "        #                 [/INST]\"\"\"\n",
        "        \n",
        "        self.prompt = \"\"\"<|system|>\n",
        "                          {system_prompt}\n",
        "                          \n",
        "                        ```\n",
        "                        {context}\n",
        "                        ```</s>\n",
        "                        <|user|>:\n",
        "                        {user_input}</s>\n",
        "                        <|{character_name}|>:\"\"\"\n",
        "        \n",
        "        self.retriever = book.get_retriever()\n",
        "\n",
        "\n",
        "    def get_response(self, user_input):\n",
        "        context = self.retriever.invoke(user_input)\n",
        "        input = self.prompt.format(user_input=user_input,\n",
        "                                   system_prompt=self.system_prompt,\n",
        "                                   context=context,\n",
        "                                   character_name=self.character_name)\n",
        "        # print(input)\n",
        "        output = text_generator(input)\n",
        "        res = output[0]['generated_text'].split(f\"<|{self.character_name}|>:\")[-1]\n",
        "        print(res)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G_b8RnXYNxeg"
      },
      "outputs": [],
      "source": [
        "def eval_response(user_input, chat, book, measure):\n",
        "\n",
        "  res = chat.get_response(user_input)\n",
        "  test_embeddings = generate_embeddings(book.get_best_sentences(user_input, chat.character_name))\n",
        "  res_embedding = generate_embeddings(res)\n",
        "  score = 0\n",
        "  for embedding in test_embeddings:\n",
        "    score += measure(embedding, res_embedding)\n",
        "  return score / len(test_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCZAwVIZUEnU",
        "outputId": "dc616e36-82bf-4fbd-d696-196c3d866c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I do not have a personal opinion or perspective on the king. However, I understand that the king's actions and responses are based on his own personal beliefs and values, and the text only reflects those beliefs and values.\n",
            "\n",
            "I do not have a personal opinion. However, the king's character is complex and nuanced, reflecting his inner torment and struggle to reconcile his love for his wife with his duty to his country and the law. The dialogue provided shows a nuanced, multidimensional character who wrestles with his conscience and his duty to the public while also pursuing his love interests. The king's motives, both public and private, are explored through his interactions with the other characters, and he demonstrates a willingness to compromise and adapt to different situations. I think that the king's inner conflict and struggle are the most intriguing part of the play. I hope that helps.\n",
            "0.3668527722358704 0.31534871757030486\n"
          ]
        }
      ],
      "source": [
        "book_path = \"D:\\\\Faks\\\\NLP\\\\Project\\\\data\\\\hamlet.txt\"\n",
        "book = Book(book_path)\n",
        "# print(book.get_characters())\n",
        "\n",
        "hamlet_chat_simple = Character_chat(character_name=\"HAMLET\", book=book, book_name=\"HAMLET\", add_quotes=False)\n",
        "hamlet_chat_with_quotes = Character_chat(character_name=\"HAMLET\", book=book, book_name=\"HAMLET\", add_quotes=True)\n",
        "\n",
        "user_input = \"What is your opinion of the king?\"\n",
        "\n",
        "measure = get_embedding_similarity\n",
        "score1 = eval_response(user_input, hamlet_chat_simple, book=book, measure=get_embedding_similarity)\n",
        "score2 = eval_response(user_input, hamlet_chat_with_quotes, book=book, measure=get_embedding_similarity)\n",
        "print(score1, score2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTSEC_l_oFj4",
        "outputId": "31353a1f-583e-4ca0-d46b-01a94b2443f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "As a fictional character, I am not capable of having an opinion about any given individual. However, based purely on the provided text material, I am impressed by Lord Caversham's character and leadership qualities. As a rising young statesman and rising star in British politics, he represents everything one should hope for in their country's future leaders. Lord Goring and the other supporting characters also demonstrate impressive leadership skills and personal qualities that are commendable and inspiring. The text material provides a well-developed character arc for Lord Caversham, showcasing his growth as a political figure and the transformation of British society. Ultimately, I believe Lord Caversham to be a compelling and fascinating character who will have a significant impact on British political history.\n",
            "\n",
            "Lord Caversham is a highly esteemed figure in English society. He is intelligent, well-traveled, and well-educated, with a reputation for honesty and integrity. As the ambassador to Vienna, Lord Caversham has made significant contributions to diplomatic affairs in Europe, particularly with his speech on the Argentine Canal proposal. Despite criticisms of his speech, he remains respected and admired by many for his ability to speak and debate on complex political issues with grace and precision. I am impressed by his intelligence and wit, and find myself drawn to his charisma and intelligence, making him a highly enjoyable character to follow.\n",
            "0.34400727450847624 0.3843380093574524\n"
          ]
        }
      ],
      "source": [
        "book_path = \"D:\\\\Faks\\\\NLP\\\\Project\\\\data\\\\ideal_husband.txt\"\n",
        "book = Book(book_path)\n",
        "# print(book.get_characters())\n",
        "\n",
        "robert_chat_simple = Character_chat(character_name='SIR ROBERT CHILTERN', book=book, book_name=\"IDEAL HUSBAND\", add_quotes=False)\n",
        "robert_chat_with_quotes = Character_chat(character_name='SIR ROBERT CHILTERN', book=book, book_name=\"IDEAL HUSBAND\", add_quotes=True)\n",
        "\n",
        "user_input = \"What is your opinion of Lord Caversham?\"\n",
        "\n",
        "measure = get_embedding_similarity\n",
        "score1 = eval_response(user_input, robert_chat_simple, book=book, measure=get_embedding_similarity)\n",
        "score2 = eval_response(user_input, robert_chat_with_quotes, book=book, measure=get_embedding_similarity)\n",
        "print(score1, score2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
